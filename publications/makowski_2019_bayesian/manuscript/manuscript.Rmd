---
title: "Indices of Effect Existence and Significance in the Bayesian Framework"
output:
  word_document:
    toc: false
    toc_depth: 3
    df_print: "kable"
    highlight: "pygments"
    reference_docx: utils/Template_Frontiers.docx
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: utils/apa.csl
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = "")
knitr::opts_chunk$set(
  comment = ">",
  dpi = 300
  # ,fig.path = "figures/"
)
options(digits = 2)
```


# Abstract {.unnumbered}

There is now a general agreement that the Bayesian statistical framework is the right way to go for psychological science. However, its flexible nature is both its power and its weakness, for there is little agreement about what indices should be computed or reported. This lack of a consensual index for the existence of an effect, such as the frequentist *p*-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several indices of effect existence, provide intuitive visual representation of the "behaviour" of such indices in relationship with common sources of variance such as sample size and frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report and allow to draw recommendations for Bayesian statistics description, critical for the standardization of scientific reporting. 

# Introduction

<!-- Introducing Bayes and issues related to p-value -->

The Bayesian framework is quickly gaining popularity among psychologists and neuroscientists [@andrews2013prior]. Reasons to prefer this approach are reliability, better accuracy in noisy data, better estimation for small samples, less proneness to type I error, the possibility of introducing prior knowledge into the analysis and, critically, the intuitiveness and straightforward interpretation of results [@dienes2018four;@etz2016bayesian;@kruschke2010believe;@kruschke2012time;@wagenmakers2018bayesian;@wagenmakers2016bayesian]. On the other hand, the frequentist approach has been associated with the focus on *p*-values and null hypothesis significance testing (NHST), the misuse of which has been shown to critically contribute to the reproducibility crisis of psychological science [@chambers2014instead; @szucs2016empirical]. In response, there is a general agreement that the generalization and utilization of the Bayesian framework is a way of overcoming these issues [@benjamin2018redefine; @etz2016bayesian; @maxwell2015psychology; @wagenmakers2017need; @marasini2016use; @halsey2019reign].

<!-- when these sig indices are needed -->

While the probabilistic reasoning put forth by the Bayesian framework is pervading most of data science aspects, it is particularly well established for statistical modelling. This facet, on which psychological science massively rely, could roughly be grouped into two soft-edged categories: predictive and structural modelling. Although a statistical model can (often) serve both purposes, predictive modelling is devoted to building and finding the best model that accurately predicts a given outcome. It is centered around concepts such as fitting metrics, predictive accuracy and model comparison. At the extremum of this dimension lie deep learning models, used for their strong predictive power, sometimes at the expense of Human readability [these models have been often referred to as "black-boxes", emphasizing the difficulty to appraise their internal functioning; @burrell2016machine; @castelvecchi2016can; @snoek2012practical]. On the other side, psychologists often use simpler models (for instance, models based on the general linear framework) to explore their data. Within this framework, the goal switches from building the best model, to understanding the parameters inside the model. In reality, the pipeline often starts with predictive modelling ("what is the best model for describing the world, *i.e.*, the observed variable") and then seemingly transition to structural modelling ("given this model of the world, how do the effects, *i.e.*, the model parameters, influence the outcome"), the conclusions of which often rely on some index of effect "significance".

<!--Estimation vs. significance -->

It is important to note that the importance and focus on significance assessment might differ across fields. For instance, in applied physics or engineering, the goal of a study is often to precisely *estimate* (quantify) a given effect and its size (the question being, for example, whether a given coefficient is closer to 7 or to 9, or whether a difference is moderate or big). The Bayesian framework has demonstrated its superiority in addressing this type of questions, with its probabilistic framework leading to more accurate estimations and allowing to quantify the inherent uncertainty associated with them. On the contrary, in other fields, such as experimental psychology, the implicit aim has often been focused on *significance*: "are these two conditions different?", "is there (any) correlation between these two variables?". In other words, focus has been on being able to conclude whether or not a given parameter affects substantially the outcome with little care to its precise quantification.

<!-- Utility of significance -->

Despite a general agreement on its defects and recurring waves of attacks occurring for decades [@gardner1986confidence; anderson2000null; finch2004reform; fidler2004editors; @cohen2016earth], the tenacity and resilience of the *p*-value as an index of significance is remarkable, being still widely used and taught (although some journals have taken a radical step by banning them, *e.g.*, @gill_2018; expecting to shift the perspective to effect size estimation). This endurance might be informative on how such indices, and the accompanying heuristics applied to interpret them (*e.g.*, *.05*, *.01* and *.001*), are useful and necessary for researchers to gain an intuitive (although possibly simplified) understanding of the interactions and structure of their data. Moreover, the utility of such an index is most salient in contexts where decisions must be made (*e.g.*, in medical settings). In spite of the statistical perspective, in which the absence of any hard mathematical thresholds or discrete categories naturally leads to the desire for a nuanced and subtle view, decisionners might often seek a threshold value to rationalize their acts and decisions. This practical need for significance assessment might be one of the mechanism fostering the wide adoption of the frequentist *p*-value and its arbitrary interpretation clusters. Unfortunately, these heuristics have severely rigidified, with meeting significance threshold becoming a goal unto themselves rather than a tool for understanding the data [@cohen2016earth; @kirk1996practical]. This is particularly problematic given that *p*-values and NHST can only be used to reject the null hypothesis and cannot be used to accept the null as true [@wagenmakers2007practical], thus making "discovery of effects" the de-facto focus of scientific research.

<!-- How to improve sig testing -->

While significance testing (and its inherent categorical interpretation heuristics) might have its place as a complementary perspective to effect estimation, it does not preclude the fact that drastic improvements are needed. For instance, one possible advance could focus on improving the mathematical understanding (*e.g.*, through a new simpler index) of the values (as opposed to the obscure mathematical definition of the *p*-value that contributes to its common misinterpretation). Another improvement could be found in providing an intuitive (*e.g.*, visual) understanding of the behaviour of the indices in relationship with main sources of variance, such as sample size, noise or effect presence. Such better overall understanding of the indices would hopefully act as a barrier against their mindless reporting by allowing the users to nuance the interpretations and conclusions that they draw.


<!--Bayesian alternatives -->

So what does the Bayesian framework offer as alternatives for the *p*-value? Bayesian testing indices could be roughly grouped into three overlapping categories: Bayes factors, posterior indices and ROPE-based indices. Bayes factors are a family of indices of relative evidence of one model over another <!-- MSB: or more generally, one parameter-sapve over another --> [*e.g.*, the null vs. the alternative hypothesis; @jeffreys1998theory; @ly2016harold]. They provide many advantages over the *p*-value by having a straightforward interpretation as well as allowing to quantify evidence in favour of the null hypothesis [@dienes2014using; @jarosz2014odds]. Nonetheless, its use for parameters description in complex models is still a matter of debate [@wagenmakers2010bayesian; @heck2019caveat], and its use is highly dependent on the specification of priors of both compared models [@etz2018bayesian; @kruschke2018bayesian]. On the contrary, "posterior indices" reflect objective characteristics of the posterior distribution, for instance the proportion of strictly positive values. While the simplicity of their computation and interpretation is an asset, it also means they are limited in the information that they provide. Importantly, Bayes factors and posterior indices are both the "natural, direct, and unavoidable consequence of Bayes' rule" [@rouder2018bayesian, p. 106]. Finally, ROPE-based indices are related to the redefinition of the null hypothesis from the classic point-null hypothesis to a range of values considered negligible, or too small to be of any practical relevance (the Region of Practical Equivalence - ROPE; @kruschke2014doing; @lakens2017equivalence; @lakens2018equivalence), usually spread equally around 0 (*e.g.*, [-0.1; 0.1]). It is interesting to note that this perspective unites Bayesian indices with the focus on effect size (involving a discrete separation between at least two categories: negligible and non-negligible), which finds an echo in recent statistical recommendations [@sullivan2012using; @ellis2003practical; @simonsohn2014p].

<!-- Research gap statement -->

Curiously, despite the richness provided by the Bayesian framework and the availability of multiple indices, no consensus has yet emerged on the ones to be used. On the contrary, the literature continues to bloom in a raging debate, often polarized between proponents of the Bayes factor as the supreme index and its detractors [*e.g.*, @spanos2013should; @robert2014jeffreys; robert2016expected; @wagenmakers2015another], with strong theoretical arguments being developed on both sides. Unfortunately, no practical, empirical and direct comparison between these indices has ever been done. This might be a deterrent for scientists interested in adopting the Bayesian framework. Moreover, this grey area can increase the difficulty of readers or reviewers unfamiliar with the Bayesian framework to follow the assumptions and conclusions, which could in turn generate unnecessary doubt upon the entire study. While we think that such indices of significance and their interpretation guidelines (in the form of rules of thumb) are useful in practice, we also strongly believe that they should be accompanied with the understanding of their "behaviour" in relationship with major sources of variance, such as sample size and noise. This knowledge is important for people to implicitly and intuitively appraise the meaning and implication of the mathematical values they report. Such an understanding could, in turn, prevent the crystallization of the possible heuristics and categories derived from such indices, as has unfortunate occurred in the used of *p*-values.

<!-- Aim and hypotheses -->

Thus, based on the simulation of multiple linear and logistic regressions (arguably some of the most widely used models in the psychological sciences), the present work aims at comparing several indices of effect "significance", provide visual representations of the "behaviour" of such indices in relationship with sample size, noise and effect presence, as well as their relationship to frequentist *p*-value (an index which, beyond its many flaws, is well known and could be used as a reference for Bayesian neophytes), and finally draw recommendations for Bayesian statistics reporting.




# Methods

## Data Simulation

The simulations of datasets suited for linear or logistic regression, we're started by simulating an independent, normally distributed *x* variable (with mean 0 and SD 1) of a given sample size. Then, the corresponding *y* variable was added, having a perfect correlation (in the case of data for linear regressions) or as a binary variable perfect separated by *x* (in the case of no effect, a *y* variable was created that was independent of *x*). Finally, a Gaussion noise was added to the *x* variable (the error).

The simulation aimed at modulating the following characteristics: *data type* (*i.e.*, used for linear or logistic regression), *Sample size* (from 20 to 100 by steps of 10), *"true" effect* (original regression coefficient from which data is drawn prior to noise addition, 1 - presence of effect or 0 - absence of effect) and *noise* (Gaussian noise applied to the predictor with SD uniformly spread between 0.666 and 6.66, with 1000 different values). We generated a dataset for each combination of these characteristics, resulting in a total of 36000 (`2 * 2 * 9 * 1000`) datasets. The code used for generation is available on Github (https://github.com/easystats/easystats/tree/master/publications/makowski_2019_bayesian/data). Please note that it takes usually several days/weeks for the generation to complete.

```{r DataLoad, message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr)
library(tidyr)
library(parameters)
library(performance)

# df <- read.csv("https://raw.github.com/easystats/easystats/master/publications/makowski_2019_bayesian/data/data.csv")
df <- read.csv("../data/data.csv")
# df <- df[seq(1,nrow(df), length.out = 3600),] # get every 10th line
```


## Indices

For each of these datasets, Bayesian and frequentist regressions were fitted to predict *y* from *x* as a single unique predictor. The Bayesian models had default, mildly informative priors (normal distribution with mean 0 and SD 1) over the parameter of interest, and were fitted using MCMC (4 chains of 2000 iterations, half of which used for warm-up). For all of the simulated models, we computed seven indices related to the effect of *x*. These are available and described in details in the *bayestestR* R package **(BAYESTESTR CITATION)**.

### Frequentist *p*-value

Based on the frequentist regression, this index represents the probability that for a given statistical model, when the null hypothesis is true, the effect would be greater than or equal to the observed coefficient [@wasserstein2016asa].

### Probability of Direction (*pd*)

The Probability of Direction (*pd*) varies between 50\% and 100\% and can be interpreted as the probability that a parameter (described by its posterior distribution) is strictly positive or negative (whichever is the most probable). It is mathematically defined as the proportion of the posterior distribution that is of the median's sign **(BAYESTESTR CITATION)**.

### MAP-based *p*-value

The *MAP-based p-value* (also referred to as the "Bayesian *p*-value") is related to the odds that a parameter has against the null hypothesis [@mills2014bayesian; @mills2017objective]. It is mathematically defined as the density value at 0 divided by the density at the Maximum A Posteriori (MAP), *i.e.*, the equivalent of the mode for continuous distributions.

### ROPE (95\%) and ROPE (full)

The *ROPE (95\%)* refers to the percentage of the 95\% HDI that lies within the ROPE. As suggested by @kruschke2014doing, the Region of Practical Equivalence (ROPE) was defined as ranging from -0.1 to 0.1 for linear regressions and its equivalent, -0.18 to -0.18, for logistic models [based on the $\pi/âˆš(3)$ formula to convert log odds ratios to standardized differences; @cohen1988statistical]. The *ROPE (full)* refers to the percentage of the whole posterior distribution that lies within the ROPE.

### Bayes factor (*vs.* 0) and Bayes factor (*vs.* ROPE)

The Bayes Factor (*BF*) used here is based on prior and posterior distributions of a single parameter. In this context, the Bayes factor indicates the degree by which the mass of the posterior distribution has shifted further away from or closer to the null value(s), relative to the prior distribution, thus indicating if the null hypothesis has become less or more likely given the observed data. We created two indices corresponding to two definitions for the null. In the case of testing against a point null (0), a Savage-Dickey density ratio was computed, which is also an approximation of a Bayes factor comparing the marginal likelihoods of the model against a model in which the tested parameter has been restricted to the point null [@wagenmakers2010bayesian]. We also computed a *BF* against the range of negligible values (defined here same as for the ROPE indices), by comparing the prior and posterior odds of the parameter falling within vs. outside the ROPE [see *Non-overlapping Hypotheses* in @morey2011bayesinterval].

## Data Analysis

The aim of this study is two-fold: 1) compare between Bayesian indices of effect existence and significance, 2) provide visual guides for an intuitive understanding of the numeric values in relation with a known frame of reference (the frequentist NHST framework). Thus, we will start by 1) presenting the relationship between these indices and main sources of variance, such as effect existence, sample size and noise. 2) Compare Bayesian indices with the frequentist *p*-value and its commonly used thresholds (.05, .01, .001). Taken together, these results will help us to outline numeric guides to ease the reporting and interpretation of the indices.

# Results

## Impact of Sample Size

```{r Figure1, message=FALSE, warning=FALSE, fig.cap="Figure 1. Impact of Sample Size.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}
knitr::include_graphics("figures/Figure1.png")
```

```{r Table1, message=FALSE, warning=FALSE, echo=FALSE}
source("make_tables.R")
knitr::kable(table1, digits = 3, caption = "Table 1. Sensitivity to sample size.")
```

The analysis suggests that ROPE-based indices are the most sensitive to sample size, followed by the MAP-based *p*, the *pd*, the *p*-value. The BF (vs. 0) and the BF (vs. ROPE) are the less sensitive to sample size. This pattern is evident across model types. Note that the *p*-value and *pd* are only affected by sample size when an effect is present, but *not* in the absence of an effect, whereas the ROPE-based indices and the Bayes factors are affected by sample size 


<!-- MSB: Move discussion of consistency here? -->


## Impact of Noise


```{r Figure2, message=FALSE, warning=FALSE, fig.cap="Figure 2. Impact of Noise.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}
knitr::include_graphics("figures/Figure2.png")
```

```{r Table2, message=FALSE, warning=FALSE, echo=FALSE}

knitr::kable(table2, digits = 3, caption = "Table 2. Sensitivity to noise.")
```

<!-- {MSB: needs to be re-done in light of new figure and structuring of no-effect -> noise = Inf } The analysis suggests that, in the absence of an effect, posterior indices (*pd*, *p*-value and MAP-based *p*) are impacted by noise to a greater extent than ROPE-based indices and BFs. In the presence of an effect, the MAP-based *p*, followed by the ROPE (full) and ROPE (95\%) are the most sensitive to noise. The *pd*, the *p*-value and finally the BFs are the less sensitive to noise. This pattern is consistent across model types. -->



## Presence *vs.* Absence of Effect


```{r Table3, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(table3, digits = 2, caption = "Table 3. Performance comparison of the indices in predicting the presence of an effect.")
```


For each index and each model type, we fitted a (frequentist) logistic regression to predict the presence or absence of effect, adjusted for noise and sample size. The comparison of the performance of these models (AIC, BIC and Tjur's R2) revealed a consistent pattern across model type (*i.e.*, similar for linear and logistic models), suggesting that *BF (vs. ROPE)* is the best index to discriminate between the presence and the absence of an effect, followed by *BF (vs. 0)*, *ROPE (full)*, *ROPE (95\_%)*, *p-MAP*, *p-direction* and the frequentist *p*-value. The Bayes factor [against the frequentist *p*-value model, computed via BIC approximation @wagenmakers2007practical], used here as a measure of relative performance, supported this conclusion.

<!-- MSB: Looking at the models, we can talk about how p-values have some discriminatory power, but how the other indices are much better (besides pd) -->

<!-- MSB: interval BFs discriminate better between null (or negligible) and true (un-negligible) effects [@morey2011bayesinterval; rouder2012default] -->


## Relationship with the frequentist *p*-value



```{r Figure3, message=FALSE, warning=FALSE, fig.cap="Figure 3. Relationship with the frequentist p-value.", fig.align='center', fig.width=21/2, fig.height=29.7/2, echo=FALSE}

knitr::include_graphics("figures/Figure3.png")

```

<!-- MSB: in each plot, the markers on the top (null effect) and bottom (true effect) represent the density of p-values -->
<!-- MSB: whereas on the right (null effect) and left (true effect) they represent the density of the index of interest -->

**Figure 3** suggests that the *pd* has a 1:1 correspondence with the frequentist *p*-value (through the formula $p_{two-sided} = 2*(1-p_d)$). *BF* indices still appear as having a strong relationship (although severely non-linear) with the frequentist index, which is to be expected since the *BF* is consistent (i.e., it approaches the "true" bound as sample size increases) both when the null is true and when the alternative is ture (as can be seen in the marginal distributions, marked by colored dashes in the plot margins), whereas the *p*-value is only consistent when the alternative is true, but has a uniform distribution [0-1] when the null is true [@rouder2012default; @rouder2009bayesian]. *ROPE*-based percentages appear to be only weakly related to *p*-values.



TODO: Add points type as sample size as ROPE seems to be directly related to groups of sample size.



## Relationship between ROPE (full), pd and BF (vs. ROPE)

```{r Figure5, message=FALSE, warning=FALSE, fig.cap="Figure 5. Relationship between three Bayesian indices.", fig.align='center', fig.width=21/1.8, fig.height=29.7/1.8, echo=FALSE}

knitr::include_graphics("figures/Figure5.png")
```


The **Figure 5** suggests that the relationship between the *ROPE (full)* and the *pd* might be strongly affected by the sample size, and the relationship between *BF (vs. ROPE)* and the *pd* might be subject to differences across model types (though see next paragraph). Moreover, the *ROPE (full)* and the *BF (vs. ROPE)* seem very closely related within the same model type. These results reflect *ROPE (full)* and *BF (vs. ROPE)*'s consistency both when the null is true and when the alternative is true, where the *pd*, being equivalent to the *p*-value, is only consistent when the null is true.


<!-- ROPE and BF -->
Note also the similarity between *ROPE (full)* and *BF (vs. ROPE)*; this is expected as mathematically, the relationship between *BF (vs ROPE)* and *ROPE (full)* is:
$$BF_{rope} = odds(ROPE_{full}) \times \frac{1}{c} = \frac{ROPE_{full}}{1-ROPE_{full}}\times \frac{1}{c}$$
Where $c$ is the *prior* odds of $\theta$ falling within the ROPE. Since this odds-ratio is dependent only on the shape of the prior distributions and the definition of the range of the ROPE, it is a constant that *normalizes* the posterior ROPE odds relative to the prior ROPE odds. This is also why *ROPE (full)* and *BF (vs. ROPE)* were found to have overall the same sensitivity in discriminating between null and true effects (but different criterion for decision making - not discussed here). <!-- MSB: we're not discussing the Bayesian indices thresholds... -->


# Discussion



```{r table4, message=FALSE, warning=FALSE, echo=FALSE}
table4 <- rbind(
  data.frame(
    "Index" = "Probability of Direction (pd)",
    "Interpretation" = "Probability that an effect is of the same sign as the median's.",
    "Definition" = "Proportion of the posterior distribution of the same sign than the median's.",
    "Strengths" = "Straightforward computation and interpretation. Objective property of the posterior distribution. 1:1 correspondence with the frequentist p-value.",
    "Limitations" = "Limited information favouring the null hypothesis."
    ),
  data.frame(
    "Index" = "MAP-based p-value",
    "Interpretation" = "Relative odds of the presence of an effect against 0.",
    "Definition" = "Density value at 0 divided by the density value at the mode of the posterior distribution.",
    "Strengths" = "Straightforward computation. Objective property of the posterior distribution",
    "Limitations" = "Limited information favouring the null hypothesis. Relates on density approximation. Indirect relationship between mathematical definition and interpretation."
    ),
  data.frame(
    "Index" = "ROPE (95%)",
    "Interpretation" = "Probability that the credible effect values are not negligible.",
    "Definition" = "Proportion of the 95% CI inside of a range of values defined as the ROPE.",
    "Strengths" = "Provides information related to the practical relevance of the effects.",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors. Not sensitive to highly significant effects."
    ),
  data.frame(
    "Index" = "ROPE (full)",
    "Interpretation" = "Probability that the effect possible values are not negligible.",
    "Definition" = "Proportion of the posterior distribution inside of a range of values defined as the ROPE.",
    "Strengths" = "Provides information related to the practical relevance of the effects.",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors."
    ),
  data.frame(
    "Index" = "Bayes factor (vs. 0)",
    "Interpretation" = "The degree by with the probability mass has shifted away from or towards the null value, after observing the data.",
    "Definition" = "Ratio of the density of the null value between the posterior and the prior distributions.",
    "Strengths" = "An unbounded continuous measure of relative evidence. Allows statistically supporting the null hypothesis.",
    "Limitations" = "Sensitive to selection of prior distribution shape, location and scale."
    ),
  data.frame(
    "Index" = "Bayes factor (vs. ROPE)",
    "Interpretation" = "The degree by with the probability mass has shifted away from or towards the null interval (ROPE), after observing the data.",
    "Definition" = "Ratio of the odds of the posterior vs the prior distribution falling inside of the range of values defined as the ROPE.",
    "Strengths" = "An unbounded continuous measure of relative evidence.Allows statistically supporting the null hypothesis. Compared to the BF (vs. 0), evidence is accumulated faster for the null when the null is true.",
    "Limitations" = "A ROPE range needs to be arbitrarily defined. Sensitive to the scale (the unit) of the predictors. Sensitive to selection of prior distribution shape, location and scale."
    )
)

knitr::kable(table4, digits = 2, caption = "Table 4. Summary of Bayesian Indices of Effect Existence and Significance.")
```



<!-- /// Summary of aim and methods -->

Based on the simulation of multiple linear and logistic models, the present work aimed at comparing several Bayesian indices of effect existence, provide visual representations of the "behaviour" of such indices in relationship with important sources of variance such as sample size, noise, presence of effect, as well as comparing them with the well-known and widely used frequentist *p*-value and its arbitrary interpretation heuristics.


<!-- /// Results discussion -->
<!-- Sensibility to sample size -->

The results tend to suggest that indices could be separated into two categories. Consistency talk.

<!-- Sensibility to noise -->

Noise.

<!-- Sensibility to effect existence -->

Discrimination analysis (see **Table 4**). Performance talk.




<!-- Comparison to p-value -->

What is the point of comparing Bayesian indices with the frequentist *p*-value, especially after having pointed out to its many flaws? Indeed, while this comparison may seem counter-intuitive or wrong (as the Bayesian thinking is intrinsically different from the frequentist framework), we believe that this juxtaposition is interesting for didactic reasons. The frequentist *p*-value "speaks" to many and can thus be seen as a reference and a way to facilitate the shift toward the Bayesian framework. Thus, pragmatically documenting such bridges can only foster the understanding of the methodological issues that our field is facing, and in turn act against the sectarism and isolation caused by a dogmatic approach to a framework. This does not preclude, however, that a change in the general paradigm of significance seeking in necessary, and that Bayesian indices are fundamentally different from the frequentist *p*, rather than mere approximations or equivalents.


<!-- /// General discussion -->
<!-- Significance vs. Existence -->

Critically, while the purpose of these indices was solely termed as *significance* until now, we would like to emphasize the nuanced perspective of the existence-significance testing as a dual-framework for parameters description and interpretation. The idea supported here is that there is a conceptual and practical distinction, and possible dissociation to be made, between an effect's *existence* and *significance*. In this context, existence is simply defined as the consistency of an effect in one particular direction (*i.e.*, positive or negative), without any assumptions or conclusions as to its size, importance or meaning. It is an objective feature of an estimate (tied to its uncertainty). On the other hand, *significance* would be here re-framed following its original literally definition ("being worthy of attention; importance"), which a neutral approach would link with the concept of effect size. An effect can be considered significant if its magnitude is higher than a given threshold. This aspect can be explored, to a certain extent, in an objective way with the concept of *practical equivalence* [@kruschke2014doing; @lakens2017equivalence; @lakens2018equivalence, which suggests the use of a range of values assimilated to absence of effect (the ROPE). If the effect falls within this range, it is considered as non-significant *for practical reasons*: the magnitude of the effect is likely to be too small to be of paramount importance in real-world scenarios. Nevertheless, *significance* also withholds are more subjective aspect, corresponding to its contextual meaningfulness and relevance, which is highly dependent on the literature, priors, novelty, context or field, and thus cannot be neutrally assessed with any statistical index. 

Interestingly, the weight of one or the other aspect of the EXIT framework might depend on the question asked. For instance, in a study exploring the effects of a new treatment, the focus might be *existence*: how much are we certain that the effect is beneficial and not harmful? In a further step, however, the researcher might become interested in *significance*: is this effect large enough to be of any interest? Note that indices of significance and existence are conceptually independent. For example, an effect for which the whole posterior distribution is concentrated within the [0.0001, 0.0002] range would be considered as positive with a high certainty (and thus, *existing* in a that direction), but also not significant (*i.e.*, too small to be of any practical relevance). Acknowledging the distinction and complementary of these two aspects can in turn enrich the information and usefulness of the results reported in psychological science. For practical reasons, the implementation of EXIT (Effect eXistence and sIgnificance Testing) is made straightforward through the *bayestestR* open-source package for R (**BAYESTESTR CITATION**).

<!-- DM: NHST and CET are dead, long live the EXIT (Existence sIgnificance Testing) [literally just made that up, a better acronym might possibly be found ^^] -->

Critically to the aim of that paper, the EXIT dual-perspective spontaneously stems out from the probabilistic nature of the Bayesian framework, which allows these two aspects of parameters assessment to coexist and yet be neatly delineated. Moreover, the distinction between *existence* and *significance* is also supported by the empirical data presented in this paper, in regards to the sensitivity to the indices to the amount of evidence (sample size). In this context, the *pd* and the MAP-based *p* appears as indices of effect existence, mostly sensitive to the certainty related to the direction of the effect and ROPE-based indices and Bayes factors are effect of significance (related to the magnitude and the amount of evidence in favour of it). Thus, an effect will be comprehensively reported if, beyond its estimation [with a point estimate, such as the median, and an index of uncertainty, such as the 89\% Credible Interval; @mcelreath2018statistical], it presents an index of existence and contextually test and discuss its significance and relevance in regards to theoretically justified characteristics.


<!-- ROPE -->
The inherent subjectivity related to the assessment of significance is one of the limitation the ROPE-based indices (although, conceptually, a strength, allowing for contextual nuance in the interpretaion), as they require an explicit definition of the non-significant range (the ROPE). Although default values were reported in the litterature [for instance, half of a "negligible" effect size reference value; @kruschke2014doing], it is critical for the reproducibility and transparency that the researcher's choice is explicitly stated (and if possible, justified). Beyond being arbitrary, this range also has hard bounds: For instance, contrary to a value of 0.0499, a value of 0.0501 will be considered as non-negligible. This reinforces a categorical and clustered perspective of what is by essence continuous space of possibilities. Importantly, as this range is fixed to the scale of the outcome response (in is expressed in the unit of the outcome), these indices are sensitive to changes in the scale of the predictors. In other words, as the ROPE represents a fixed portion of the response's scale, it is dependent on the scale of the predictor. For instance, in the case of a simple linear regression, for which the median of the coefficient of *x* on *y* is of 0.02 and does within the ROPE (being not significant), simply multiplying *x* values by 100 would result in a coefficient with a meidan of 0.02 * 100 = 20 and would fall outside of the rope (which range is fixed), that one inatentive or malicious researcher could 
unreasonably present as "significant" (note that indices of existence, such as *pd*, would not be affected). Finally, the ROPE definition is also dependent on the model type, and selecting a consistent or homogenous range for all the families of models is not straightforward. This, in turn, can make comparisons between model types difficult, and an additional burden when interpretating ROPE-based indices. While in general, a well-defined ROPE is a powerful tool to give a different and new perspective, it also requires extra caution from the authors and the readers.
<!-- - ROPE seems also strongly affected by sample {MSB: this is a feature - consistency!} -->

As for the difference between ROPE (95\%) and ROPE (full), we suggest reporting the latter (*i.e.*, the percentage of the whole posterior distribution that falls within the ROPE instead of a given proportion of CI). This bypass the usage of another arbitrary range (95\%) and appears to be more sensitive to delineate highly significant effects). Critically, rather than using the percentage in ROPE as a dichotomous, all-or-nothing decision criterion, such as suggested by the original equivalence test [@kruschke2014doing], we recommend using the percentage as a continuous index of significance.


<!-- BFs -->
Strengths and weaknesses.


<!-- The pd and p-MAP -->
The Probability of Direction (pd) is an index of effect existence, ranging from 50\% to 100\%, representing the certainty with which an effect goes in a particular direction (*i.e.*, is positive or negative). Beyond its simplicity of interpretation, understanding and computation, this index also presents other interesting properties. It is independent from the model, *i.e.*, it is solely based on the posterior distributions and does not require any additional information from the data or the model. Contrary to ROPE-based indices, it is robust to the scale of both the response variable and the predictors. Nevertheless, this index also presents some limitations. Most importantly, the *pd* is not relevant to assess size or importance and is not enable to give information in favour of the null. In other words, a high *pd* suggests the presence of an effect but a small *pd* does not give us any information about how much the null hypothesis is plausible, suggesting that this index can only be used to eventually "reject the null" (but not accepting it, which is consistent with the interpretation of the frequentist *p*-value). On the contrary, the BFs (and to some extent the percentage in ROPE, although being bounded to 0\% and 100\%) continue increasing or decreasing as the evidence becomes stronger (more datapoints), in both directions. 

Much of these strengths also apply to the MAP-based *p*-value. Altough possibly showing some superiority in terms of sensitivity as compared to the *pd*, we would still favour the latter. Indeed, the MAP is mathematically dependent on the density at 0 and at the mode. However, the density estimation of a continuous distribution is a statistical problem on its own and many different methods exist. It is possible that changing the density estimation might impact the MAP-based *p*-value with unknown results. Additionaly, the *pd* has a linear relationship with the frequentist *p*-value, which is in our opinion an asset.

After all the criticism regarding the frequentist *p*-value, it might appear as counter-intuitive to suggest the usage of its Bayesian empirical equivalent. The more subtle perspective that we support is that the *p*-value is not an intrinsically bad, or wrong, index. Instead, it is its misuse, misunderstanding and misinterpretation that fuels, in our opinion, the decay of the situation into the crisis. Interestingly, the proximity between the *pd* and the the *p*-value suggests that the latter is more an index of effect *existence* than *significance* (*i.e.*, "worth of interest"). Addressing this confusion, the Bayesian equivalent has an intuitive meaning and interpretation, making also obvious the fact that all thresholds and heuristics are arbitrary. Additionally, its mathematical and interpretative transparency of the *pd*, and its conceptualization as an index of effect existence, offers a valuable insight into the characterization of Bayesian results, and its practical proximity with the frequentist *p*-value makes it a perfect metric to ease the transition of psychological research into the adoption of the Bayesian framework.




# Reporting Guidelines

How these observations can be used to improve statistical good practices in psychological science? Importantly, before being able to draw a definitive conclusion about the qualities of these indices, further studies need to investigate the robustness of these indices to sampling characteristics (*e.g.*, sampling algorithm, number of iterations, chains, warm-up) and the impact of prior specification [@kass1995bayes; @vanpaemel2010prior; @kruschke2011bayesian], all of which are important parameters of Bayesian statistics. <!-- MSB: All of these refs are about the BF, as ROPE etc are not affected by the how "wide" the priors are, but instead by how "narrow" they are - what I called the reversed Jeffreys-Lindley-Bartlett paradox: https://github.com/easystats/blog/issues/25#issuecomment-503391174 -->
<!-- DM: But we don't have any citation for ROPE? I would add something about the reversed paradox to make it an official concept hehe :) -->

Nevertheless, based on the present comparison, we can start outlining the following guidelines. As the two aspects of the EXIT framework, *existence* and *significance*, are complementary, we suggest using at minimum one index of each category. Thus, in order to assess the existence and significance of effects within a regression model, we recommend to report, at minimum, the *pd* as an objective index of effect existence and the *BF (vs. ROPE)* as an index of significance. The former for its simplicity of interpretation, its robustness and its numeric proximity to the well-known frequentist *p*-value, and the latter for its ability to discriminate between presence and absence of effect [@de2007alternative], and the information it provides related to relative evidence of the size of the effect. 
<!-- MSB: BF-ROPE is most sensitive to effect **existence** as found in the sensitivity analysis! And IMO, it is also simple to interp -->
<!-- Dom: depends what you call existence I guess... But still, since ROPE is inherently defining a range based on size (negligible), and BF ROPE is testing the odds in favour/against this range (i.e., the effect being non-negligible vs. the effect being negligible), thus it is conceptually an index of effect significance rather than merely existence (as defined in EXIT) -->

Defining appropriate heuristics to help the interpretation is beyond the scope of this study, as it would require testing them on more natural datasets. Nevertheless, if we take the frequentist framework and the existing literature as a reference point, it seems that 95\%, 97\% and 99\% might be relevant reference points (*i.e.*, easy-to-remember values) for the *pd* and 3, 10 and 30 (weak evidence) appropriate for the BF. A concise, standardized, reference template sentence to describe the parameter of a model including an index of point-estimate, uncertainty, existence, significance and effect size [@cohen1988statistical] could be:

"The effect of X has a probability of 92.14\% [*pd*] of being negative (Median = -0.03, 89\% CI [-0.05, 0.01]), moderately not-significant (BF<sub>ROPE</sub> = 0.29) [*BF (vs. ROPE)*] and can be considered as very small (Std. Median = -0.09) [*standardized coefficient*]."


# Supplementary Materials

The full R code used for data generation, data processing, figures creation and manuscript compiling is available on Github at https://github.com/easystats/easystats/tree/master/publications/makowski_2019_bayesian.

# Acknowledgments

This study was made possible by the development of the **bayestestR** package, itself part of the [*easystats*](https://github.com/easystats/easystats) ecosystem, an open-source and collaborative project created to facilitate the usage of R. Thus, we would like to thank the [council of masters](https://github.com/orgs/easystats/people) of easystats, all other padawan contributors, as well as the users.


# References
